{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ƒê·ªì √°n m√¥n h·ªçc**\n",
    "## M√¥n: L·∫≠p tr√¨nh song song ·ª©ng d·ª•ng\n",
    "### L·ªõp CQ2021/21 - Nh√≥m 05\n",
    "#### Th√†nh vi√™n:\n",
    "| H·ªç v√† t√™n             | MSSV |\n",
    "| :-----------          |     :----:|\n",
    "| Di·ªáp ƒê·∫°i Thi·ªán Nh√¢n | 18120491  |\n",
    "| Ho√†ng Trung Nam  | 21120290 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **T·ªïng quan ƒë·ªì √°n**\n",
    "\n",
    "- Nh√≥m quy·∫øt ƒë·ªãnh th·ª≠ s·ª©c v·ªõi VGG.\n",
    "\n",
    "- VGG l√† m·ªôt ki·∫øn tr√∫c CNN s√¢u ƒë∆∞·ª£c gi·ªõi thi·ªáu b·ªüi nh√≥m VGG (Visual Geometry Group) thu·ªôc ƒê·∫°i h·ªçc Oxford nƒÉm 2014. Ki·∫øn tr√∫c n√†y n·ªïi b·∫≠t b·ªüi vi·ªác s·ª≠ d·ª•ng nhi·ªÅu l·ªõp convolution 3x3 li√™n ti·∫øp thay v√¨ m·ªôt l·ªõp convolution l·ªõn (vd: 5x5 hay 7x7), nh·∫±m tƒÉng kh·∫£ nƒÉng h·ªçc ƒë·∫∑c tr∆∞ng v√† gi·ªØ k√≠ch th∆∞·ªõc receptive field ·ªïn ƒë·ªãnh.\n",
    "\n",
    "- Hai phi√™n b·∫£n ph·ªï bi·∫øn:\n",
    "  - VGG-16: 16 l·ªõp c√≥ tr·ªçng s·ªë (13 conv + 3 FC).\n",
    "  - VGG-19: 19 l·ªõp c√≥ tr·ªçng s·ªë (16 conv + 3 FC).\n",
    "\n",
    "- **Input c·ªßa CIFAR-10** l√† `32x32x3`, nh·ªè h∆°n r·∫•t nhi·ªÅu so v·ªõi `224x224x3` c·ªßa VGG g·ªëc.\n",
    "\n",
    "V·∫≠y n√™n, khi √°p d·ª•ng VGG cho CIFAR-10, ta c·∫ßn **gi·∫£m s·ªë l∆∞·ª£ng layer** ho·∫∑c **ƒëi·ªÅu ch·ªânh input size** nh∆∞ sau:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Ki·∫øn tr√∫c m√¥ h√¨nh c·ª• th·ªÉ**\n",
    "\n",
    "- **Input Layer**  \n",
    "  - 32√ó32√ó3 (RGB)\n",
    "\n",
    "- **Block 1**  \n",
    "  - Conv3√ó3, 64 filters + ReLU  \n",
    "  - Conv3√ó3, 64 filters + ReLU  \n",
    "  - MaxPool 2√ó2\n",
    "\n",
    "- **Block 2**  \n",
    "  - Conv3√ó3, 128 filters + ReLU  \n",
    "  - Conv3√ó3, 128 filters + ReLU  \n",
    "  - MaxPool 2√ó2\n",
    "\n",
    "- **Block 3**  \n",
    "  - Conv3√ó3, 256 filters + ReLU  \n",
    "  - Conv3√ó3, 256 filters + ReLU  \n",
    "  - MaxPool 2√ó2\n",
    "\n",
    "- **Fully Connected**  \n",
    "  - FC 512 + ReLU  \n",
    "  - FC 10 + Softmax\n",
    "\n",
    "- **Optimization**  \n",
    "  - Loss: Cross‚ÄëEntropy  \n",
    "  - Optimizer: SGD/Adam  \n",
    "  - Parallelization: Vi·∫øt b·∫±ng Numpy ‚Üí t·ªëi ∆∞u v·ªõi Numba (`@cuda.jit`)\n",
    "\n",
    "Ki·∫øn tr√∫c VGG‚Äëlike n√†y gi·ªØ tinh th·∫ßn ‚Äúnhi·ªÅu conv 3√ó3‚Äù c·ªßa VGG g·ªëc nh∆∞ng thu g·ªçn cho ph√π h·ª£p v·ªõi k√≠ch th∆∞·ªõc ·∫£nh 32√ó32 v√† dataset CIFAR‚Äë10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dataset**\n",
    "\n",
    "- **CIFAR-10**:\n",
    "  - 60.000 ·∫£nh m√†u 32√ó32 thu·ªôc 10 l·ªõp.\n",
    "\n",
    "  - 50.000 ·∫£nh train, 10.000 ·∫£nh test.\n",
    "\n",
    "  - Link: https://www.cs.toronto.edu/~kriz/cifar.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **N·ªôi dung ƒë√£ ho√†n th√†nh**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ho√†n th√†nh code Python tu·∫ßn t·ª± => k·∫øt qu·∫£ ch·∫°y qu√° ch·∫≠m d√π ch·ªâ test v·ªõi 10 ·∫£nh v√† 1 epoch m√† th·ªùi gian l√™n t·ªõi 15 ph√∫t.\n",
    "\n",
    "- Ho√†n th√†nh c√°c class c∆° b·∫£n v√† class c·∫•u tr√∫c c·ªßa Numpy, s·∫Ω ti·∫øn h√†nh ch·∫°y th·ª≠ v√†o tu·∫ßn t·ªõi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **C√†i ƒë·∫∑t h√†m tu·∫ßn t·ª±**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **√ù t∆∞·ªüng**\n",
    "\n",
    "#### üì• Input Layer\n",
    "\n",
    "- K√≠ch th∆∞·ªõc ·∫£nh: **32 √ó 32 √ó 3 (RGB)**  \n",
    "- Sau m·ªói layer, k√≠ch th∆∞·ªõc thay ƒë·ªïi t√πy theo padding v√† pooling.\n",
    "\n",
    "---\n",
    "\n",
    "#### üß† 1. Convolutional Layer (3√ó3)\n",
    "\n",
    "**C√¥ng th·ª©c:**\n",
    "\n",
    "$$\n",
    "Y[i,j,cout] = \\sum_{u=0}^{2} \\sum_{v=0}^{2} \\sum_{cin=0}^{C_{in}-1} K[u,v,cin,cout] \\cdot X[i+u, j+v, cin]\n",
    "$$\n",
    "\n",
    "**Ch√∫ th√≠ch bi·∫øn:**\n",
    "- $ X $ : ·∫£nh ƒë·∫ßu v√†o, k√≠ch th∆∞·ªõc (H, W, Cin)\n",
    "- $K$: kernel/l·ªçc, k√≠ch th∆∞·ªõc (3, 3, Cin, Cout)\n",
    "- $Y$: ·∫£nh ƒë·∫ßu ra sau convolution, k√≠ch th∆∞·ªõc (H', W', Cout)\n",
    "- $i, j$: v·ªã tr√≠ kh√¥ng gian\n",
    "- $u, v$: t·ªça ƒë·ªô trong kernel\n",
    "- $cin, cout$: ch·ªâ s·ªë k√™nh ƒë·∫ßu v√†o v√† ƒë·∫ßu ra\n",
    "\n",
    "**Song song h√≥a:**  \n",
    "‚úî M·ªói ph·∫ßn t·ª≠ $Y[i,j,cout]$ ƒë·ªôc l·∫≠p ‚Üí tri·ªÉn khai b·∫±ng `@cuda.jit` (Numba)\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚ö° 2. Activation Layer (ReLU)\n",
    "\n",
    "**C√¥ng th·ª©c:**\n",
    "\n",
    "$$\n",
    "A[i,j,c] = \\max(0, Z[i,j,c])\n",
    "$$\n",
    "\n",
    "**Ch√∫ th√≠ch bi·∫øn:**\n",
    "- $Z$ : ƒë·∫ßu ra t·ª´ convolution\n",
    "- $A$: ƒë·∫ßu ra sau k√≠ch ho·∫°t ReLU\n",
    "\n",
    "**Song song h√≥a:**  \n",
    "‚úî M·ªói ph·∫ßn t·ª≠ ƒë·ªôc l·∫≠p ‚Üí d√πng `@vectorize` ho·∫∑c `@cuda.jit`\n",
    "\n",
    "---\n",
    "\n",
    "#### üåä 3. Max Pooling Layer (2√ó2)\n",
    "\n",
    "**C√¥ng th·ª©c:**\n",
    "\n",
    "$$\n",
    "P[i,j,c] = \\max \\begin{Bmatrix}\n",
    "X[2i, 2j, c], & X[2i+1, 2j, c], \\\\\n",
    "X[2i, 2j+1, c], & X[2i+1, 2j+1, c]\n",
    "\\end{Bmatrix}\n",
    "$$\n",
    "\n",
    "**Ch√∫ th√≠ch bi·∫øn:**\n",
    "- $X$: ƒë·∫ßu v√†o tr∆∞·ªõc pooling\n",
    "- $P$: ƒë·∫ßu ra sau pooling\n",
    "- $i, j$: v·ªã tr√≠ tr√™n ·∫£nh pooling\n",
    "- $c$ : k√™nh m√†u\n",
    "\n",
    "**Song song h√≥a:**  \n",
    "‚úî M·ªói v√πng pooling ƒë·ªôc l·∫≠p ‚Üí d√πng `@cuda.jit`\n",
    "\n",
    "---\n",
    "\n",
    "#### üß© 4. Fully Connected Layer (FC)\n",
    "\n",
    "**C√¥ng th·ª©c:**\n",
    "\n",
    "$$\n",
    "y_k = \\sum_{d=0}^{D-1} W[d,k] \\cdot x_d + b_k\n",
    "$$\n",
    "\n",
    "**Ch√∫ th√≠ch bi·∫øn:**\n",
    "- $x$: vector ƒë·∫ßu v√†o ƒë√£ flatten (sau c√°c conv)\n",
    "- $W$: ma tr·∫≠n tr·ªçng s·ªë FC (D, K)\n",
    "- $b$: bias\n",
    "- $y$: output vector\n",
    "\n",
    "**Song song h√≥a:**  \n",
    "‚úî M·ªói $y_k$ ƒë·ªôc l·∫≠p ‚Üí d√πng `@cuda.jit` ho·∫∑c `@njit(parallel=True)`\n",
    "\n",
    "---\n",
    "\n",
    "#### üî• 5. Softmax + Cross-Entropy Loss\n",
    "\n",
    "**Softmax:**\n",
    "\n",
    "$$\n",
    "p_i = \\frac{e^{y_i}}{\\sum_j e^{y_j}}\n",
    "$$\n",
    "\n",
    "**Loss:**\n",
    "\n",
    "$$\n",
    "L = -\\sum_i t_i \\cdot \\log(p_i)\n",
    "$$\n",
    "\n",
    "**Ch√∫ th√≠ch bi·∫øn:**\n",
    "- $y_i$: gi√° tr·ªã ƒë·∫ßu ra t·ª´ FC\n",
    "- $p_i$: x√°c su·∫•t sau softmax\n",
    "- $t_i$: nh√£n th·∫≠t (one-hot)\n",
    "- $L$: h√†m m·∫•t m√°t\n",
    "\n",
    "**Song song h√≥a:**  \n",
    "‚úò Kh√¥ng c·∫ßn, t√≠nh nh·∫π, c√≥ th·ªÉ x·ª≠ l√Ω tr√™n CPU\n",
    "\n",
    "---\n",
    "\n",
    "#### üîÅ 6. Backpropagation + Weight Update\n",
    "\n",
    "##### Gradient c·ªßa Softmax + CrossEntropy:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial y_i} = p_i - t_i\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "##### Gradient c·ªßa FC:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W[d,k]} = x_d \\cdot \\frac{\\partial L}{\\partial y_k}\n",
    "\\quad ; \\quad\n",
    "\\frac{\\partial L}{\\partial b_k} = \\frac{\\partial L}{\\partial y_k}\n",
    "$$\n",
    "\n",
    "**Song song h√≥a:** ‚úî t·ª´ng ph·∫ßn t·ª≠ ƒë·ªôc l·∫≠p ‚Üí `@cuda.jit`\n",
    "\n",
    "---\n",
    "\n",
    "##### Gradient c·ªßa ReLU:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial Z[i,j,c]} = \n",
    "\\begin{cases}\n",
    "\\frac{\\partial L}{\\partial A[i,j,c]}, & \\text{if } Z[i,j,c] > 0 \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "##### Gradient c·ªßa Convolution:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial K[u,v,cin,cout]} = \\sum_{i,j} \\frac{\\partial L}{\\partial Y[i,j,cout]} \\cdot X[i+u,j+v,cin]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial X[i,j,cin]} = \\sum_{u,v,cout} \\frac{\\partial L}{\\partial Y[i-u,j-v,cout]} \\cdot K[u,v,cin,cout]\n",
    "$$\n",
    "\n",
    "**Song song h√≥a:** ‚úî n·∫∑ng nh·∫•t ‚Üí d√πng `@cuda.jit` ƒë·ªÉ t·ªëi ∆∞u\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ T√≥m t·∫Øt kh·∫£ nƒÉng song song h√≥a\n",
    "\n",
    "| L·ªõp              | C·∫ßn song song | Ghi ch√∫                                |\n",
    "|------------------|----------------|----------------------------------------|\n",
    "| Convolution       | ‚úÖ              | T√≠nh to√°n n·∫∑ng, ph·∫ßn t·ª≠ ƒë·ªôc l·∫≠p         |\n",
    "| ReLU              | ‚úÖ              | M·ªói ph·∫ßn t·ª≠ t√≠nh ri√™ng ƒë∆∞·ª£c             |\n",
    "| Max Pooling       | ‚úÖ              | M·ªói v√πng pooling ƒë·ªôc l·∫≠p               |\n",
    "| Fully Connected   | ‚úÖ              | M·ªói neuron ƒë·∫ßu ra ƒë·ªôc l·∫≠p              |\n",
    "| Softmax + Loss    | ‚ùå              | T√≠nh nh·∫π, x·ª≠ l√Ω CPU v·∫´n hi·ªáu qu·∫£       |\n",
    "| Backpropagation   | ‚úÖ              | C√°c ƒë·∫°o h√†m c√≥ th·ªÉ t√≠nh ƒë·ªôc l·∫≠p        |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Chu·∫©n b·ªã d·ªØ li·ªáu**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Done.\n"
     ]
    }
   ],
   "source": [
    "# Script n√†y t·∫£i xu·ªëng v√† gi·∫£i n√©n t·∫≠p d·ªØ li·ªáu CIFAR-10 t·ª´ trang web ch√≠nh th·ª©c\n",
    "def download_cifar10():\n",
    "    url = 'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n",
    "    filename = 'cifar-10-python.tar.gz'\n",
    "    folder = 'cifar-10-batches-py'\n",
    "\n",
    "    if not os.path.exists(filename):\n",
    "        print(\"‚è¨ Downloading CIFAR-10...\")\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "    if not os.path.exists(folder):\n",
    "        print(\"üì¶ Extracting CIFAR-10...\")\n",
    "        with tarfile.open(filename, 'r:gz') as tar:\n",
    "            tar.extractall()\n",
    "    print(\"‚úÖ Done.\")\n",
    "\n",
    "download_cifar10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CIFAR-10 loaded: (50000, 3, 32, 32) (50000,)\n"
     ]
    }
   ],
   "source": [
    "# Load CIFAR-10 dataset\n",
    "def load_batch(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pickle.load(f, encoding='bytes')\n",
    "        images = data[b'data']\n",
    "        labels = data[b'labels']\n",
    "        images = images.reshape(-1, 3, 32, 32)  # N x C x H x W\n",
    "        return images, labels\n",
    "\n",
    "def load_cifar10_data():\n",
    "    base_dir = 'cifar-10-batches-py'\n",
    "    X_train, y_train = [], []\n",
    "\n",
    "    # Load 5 training batches\n",
    "    for i in range(1, 6):\n",
    "        images, labels = load_batch(f'{base_dir}/data_batch_{i}')\n",
    "        X_train.append(images)\n",
    "        y_train += labels\n",
    "\n",
    "    # Load test batch\n",
    "    X_test, y_test = load_batch(f'{base_dir}/test_batch')\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    X_train = np.concatenate(X_train, axis=0)\n",
    "    y_train = np.array(y_train)\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_cifar10_data()\n",
    "print(\"‚úÖ CIFAR-10 loaded:\", X_train.shape, y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in c√°c ·∫£nh ng·∫´u nhi√™n\n",
    "def show_random_images(X, y, num_images=5):\n",
    "    plt.figure(figsize=(10, 2))\n",
    "    for i in range(num_images):\n",
    "        idx = random.randint(0, len(X) - 1)\n",
    "        plt.subplot(1, num_images, i + 1)\n",
    "        plt.imshow(X[idx].transpose(1, 2, 0))  # Chuy·ªÉn ƒë·ªïi t·ª´ (C, H, W) sang (H, W, C)\n",
    "        plt.title(f\"Label: {y[idx]}\")\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_images(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Current directory tree:\n",
      "./\n",
      "    .gitignore\n",
      "    app_group7.ipynb\n",
      "    cifar-10-python.tar.gz\n",
      "    Image_Classification.ipynb\n",
      "    README.md\n",
      "    .git/\n",
      "        COMMIT_EDITMSG\n",
      "        config\n",
      "        description\n",
      "        FETCH_HEAD\n",
      "        HEAD\n",
      "        index\n",
      "        ORIG_HEAD\n",
      "        packed-refs\n",
      "        hooks/\n",
      "            applypatch-msg.sample\n",
      "            commit-msg.sample\n",
      "            fsmonitor-watchman.sample\n",
      "            post-update.sample\n",
      "            pre-applypatch.sample\n",
      "            pre-commit.sample\n",
      "            pre-merge-commit.sample\n",
      "            pre-push.sample\n",
      "            pre-rebase.sample\n",
      "            pre-receive.sample\n",
      "            prepare-commit-msg.sample\n",
      "            push-to-checkout.sample\n",
      "            sendemail-validate.sample\n",
      "            update.sample\n",
      "        info/\n",
      "            exclude\n",
      "        logs/\n",
      "            HEAD\n",
      "            refs/\n",
      "                heads/\n",
      "                    main\n",
      "                    nam\n",
      "                remotes/\n",
      "                    origin/\n",
      "                        HEAD\n",
      "                        main\n",
      "                        nam\n",
      "                        nhan\n",
      "        objects/\n",
      "            07/\n",
      "                9328c2a857eb6b7eb2d0aec44418331d180f91\n",
      "            0e/\n",
      "                f8504b06a892562db38a0c120e9e92d760fc65\n",
      "            11/\n",
      "                8b07b73afb12255aa0efd88e69918d0842c6d3\n",
      "            17/\n",
      "                8fd27a103cb18e5eb78a463763dc91940e269a\n",
      "            18/\n",
      "                b25703f873f2bd7c2dd2c2216fa77c18f2f418\n",
      "            1e/\n",
      "                6e3ded635faa63d475e95526990d6bf0c62bac\n",
      "            22/\n",
      "                1d68e7ec384cf54aad576ac212709a7093d8c3\n",
      "            24/\n",
      "                639e0e7873c473a4e3a58853000835a7db9e5e\n",
      "            32/\n",
      "                6fd42ae4f09ccb915af09412ae2b0419edfeed\n",
      "            38/\n",
      "                4acb1a35a53d63d0660fa0bf2c98268be5aa33\n",
      "            3c/\n",
      "                daf3d51ebdfe359cba2401e28008502968b224\n",
      "            3e/\n",
      "                6aece0376616de7e25e27af22d1c2dfa667f55\n",
      "            4b/\n",
      "                3f82c61707922e0920904105c24e591f15451e\n",
      "            55/\n",
      "                d021809338279014358745129d1ab43bffdd30\n",
      "            59/\n",
      "                70c3eb2cddd263855e60af6011357917bd6a88\n",
      "                d5545cdebe9367ad8278e9ca19a848281d05d3\n",
      "            5a/\n",
      "                501b8efae979e1372b2aeeafe8b4015fbfe3a0\n",
      "            64/\n",
      "                0973ccf4a77054c4aa0c3c817ea51e58724ccf\n",
      "                ade14fdd24bd7249a5b54be13090869d3342d8\n",
      "            6c/\n",
      "                ffef405097ede7601d642f5f9a89c8728664fd\n",
      "            75/\n",
      "                5edfef617859d07f61b381abd5a04cdcc88bd2\n",
      "                fee1bea326fa0b583bcc3d22e6fcc05181f872\n",
      "            79/\n",
      "                5cfc5e27c2409ac2fe5cd33d2dddaf5d1db597\n",
      "                bf8161988ac80f26ae596c6ea7240abe214c6b\n",
      "            7e/\n",
      "                5a57fa88e76378fcffbda151424b1d5b71e6d4\n",
      "            82/\n",
      "                420da7ef106703fc8db52563cd216941a9e7c2\n",
      "            90/\n",
      "                c5365492dea3b3c855b2375f1de8588ac1bda4\n",
      "            a0/\n",
      "                045f79b8b37b1a8154e8ddb51fcf794a021f94\n",
      "            a7/\n",
      "                194965f1884d29f9cc8269d520054734a60ed1\n",
      "            a8/\n",
      "                057ee98207924b4926f82cd9a67ddd987023cc\n",
      "            b2/\n",
      "                8d1896309bee266f1a63819bf80e5028e452ea\n",
      "            b6/\n",
      "                4bd955390f56e4149388ee141d92fedbd8503e\n",
      "            bb/\n",
      "                81b9a0568d8cb5f9b8cade97bebf57e80b0a9c\n",
      "            c2/\n",
      "                1bc6bb1080e64a3c729c4ba5c8e48a6624fa25\n",
      "            c6/\n",
      "                5b3d49bd214cded86deb7fa7b07d32f98d7c0c\n",
      "            ca/\n",
      "                2858ec00541db4aee0f113b0537e441c2da862\n",
      "            ce/\n",
      "                0dc92c97ef33fcbbfaedf7a96ef51a3c67d2f2\n",
      "            d0/\n",
      "                7a27fbcca44200e69fd2f8c8fbd089ffa1f825\n",
      "            da/\n",
      "                f5330cc66b297f2cb030679daddb968d54d030\n",
      "            db/\n",
      "                f3e6c5fee07fa0cc1fabc6027649315ec44c8d\n",
      "            e0/\n",
      "                4dcd2597373cdd003854989ea712c29cded554\n",
      "            ee/\n",
      "                60b49cdebc2f6b52ab11236fe4e29f9d2c8f27\n",
      "            fa/\n",
      "                d99d163d3de22fd4b7c9ac918aaad688c669c1\n",
      "            fb/\n",
      "                b804752bbb0ccd9c2c56f7ee0fff60cc6e358c\n",
      "            info/\n",
      "            pack/\n",
      "                pack-f2981570e88e9bd5e858267818b76b149629a302.idx\n",
      "                pack-f2981570e88e9bd5e858267818b76b149629a302.pack\n",
      "                pack-f2981570e88e9bd5e858267818b76b149629a302.rev\n",
      "        refs/\n",
      "            heads/\n",
      "                main\n",
      "                nam\n",
      "            remotes/\n",
      "                origin/\n",
      "                    HEAD\n",
      "                    main\n",
      "                    nam\n",
      "                    nhan\n",
      "            tags/\n",
      "    cifar-10-batches-py/\n",
      "        batches.meta\n",
      "        data_batch_1\n",
      "        data_batch_2\n",
      "        data_batch_3\n",
      "        data_batch_4\n",
      "        data_batch_5\n",
      "        readme.html\n",
      "        test_batch\n"
     ]
    }
   ],
   "source": [
    "# in c√¢y th∆∞ m·ª•c hi·ªán t·∫°i\n",
    "def print_directory_tree(path='.'):\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        level = root.replace(path, '').count(os.sep)\n",
    "        indent = ' ' * 4 * (level)\n",
    "        print(f\"{indent}{os.path.basename(root)}/\")\n",
    "        subindent = ' ' * 4 * (level + 1)\n",
    "        for f in files:\n",
    "            print(f\"{subindent}{f}\")\n",
    "\n",
    "print(\"üìÇ Current directory tree:\")\n",
    "print_directory_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Phi√™n b·∫£n ƒë∆∞·ª£c vi·∫øt b·∫±ng python thu·∫ßn**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Class Conv2D**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_pad(input_data, pad):\n",
    "        H, W, C = len(input_data), len(input_data[0]), len(input_data[0][0])\n",
    "        padded = [[[0.0 for _ in range(C)] for _ in range(W + 2 * pad)] for _ in range(H + 2 * pad)]\n",
    "        \n",
    "        for i in range(H):\n",
    "            for j in range(W):\n",
    "                for c in range(C):\n",
    "                    padded[i + pad][j + pad][c] = input_data[i][j][c]\n",
    "        \n",
    "        return padded\n",
    "\n",
    "class Conv2D:\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=0, stride=1):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "\n",
    "        self.kernels = [[[[random.uniform(-0.1, 0.1)\n",
    "            for _ in range(out_channels)]\n",
    "            for _ in range(in_channels)]\n",
    "            for _ in range(kernel_size)]\n",
    "            for _ in range(kernel_size)]\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        self.input_data = input_data\n",
    "        pad = self.padding\n",
    "        stride = self.stride\n",
    "        k = self.kernel_size\n",
    "        cin = self.in_channels\n",
    "        cout = self.out_channels\n",
    "\n",
    "        input_padded = zero_pad(input_data, pad)\n",
    "        self.input_padded = input_padded\n",
    "\n",
    "        H, W, _ = len(input_padded), len(input_padded[0]), len(input_padded[0][0])\n",
    "        out_h = (H - k) // stride + 1\n",
    "        out_w = (W - k) // stride + 1\n",
    "\n",
    "        output = [[[0.0 for _ in range(cout)] for _ in range(out_w)] for _ in range(out_h)]\n",
    "\n",
    "        for i in range(out_h):\n",
    "            for j in range(out_w):\n",
    "                for cout_idx in range(cout):\n",
    "                    val = 0.0\n",
    "                    for u in range(k):\n",
    "                        for v in range(k):\n",
    "                            for cin_idx in range(cin):\n",
    "                                x = i * stride + u\n",
    "                                y = j * stride + v\n",
    "                                val += input_padded[x][y][cin_idx] * self.kernels[u][v][cin_idx][cout_idx]\n",
    "                    output[i][j][cout_idx] = val\n",
    "        return output\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        pad = self.padding\n",
    "        stride = self.stride\n",
    "        k = self.kernel_size\n",
    "        cin = self.in_channels\n",
    "        cout = self.out_channels\n",
    "        input_padded = self.input_padded\n",
    "        H, W, _ = len(input_padded), len(input_padded[0]), len(input_padded[0][0])\n",
    "        out_h = len(grad_output)\n",
    "        out_w = len(grad_output[0])\n",
    "\n",
    "        grad_input_padded = [[[0.0 for _ in range(cin)] for _ in range(W)] for _ in range(H)]\n",
    "        grad_kernels = [[[[0.0 for _ in range(cout)] for _ in range(cin)] for _ in range(k)] for _ in range(k)]\n",
    "\n",
    "        for u in range(k):\n",
    "            for v in range(k):\n",
    "                for cin_idx in range(cin):\n",
    "                    for cout_idx in range(cout):\n",
    "                        grad = 0.0\n",
    "                        for i in range(out_h):\n",
    "                            for j in range(out_w):\n",
    "                                x = i * stride + u\n",
    "                                y = j * stride + v\n",
    "                                grad += input_padded[x][y][cin_idx] * grad_output[i][j][cout_idx]\n",
    "                        grad_kernels[u][v][cin_idx][cout_idx] = grad\n",
    "\n",
    "        for i in range(H):\n",
    "            for j in range(W):\n",
    "                for cin_idx in range(cin):\n",
    "                    grad = 0.0\n",
    "                    for u in range(k):\n",
    "                        for v in range(k):\n",
    "                            for cout_idx in range(cout):\n",
    "                                x = i - u\n",
    "                                y = j - v\n",
    "                                if 0 <= x // stride < out_h and 0 <= y // stride < out_w:\n",
    "                                    if x % stride == 0 and y % stride == 0:\n",
    "                                        grad += grad_output[x // stride][y // stride][cout_idx] * self.kernels[u][v][cin_idx][cout_idx]\n",
    "                    grad_input_padded[i][j][cin_idx] = grad\n",
    "\n",
    "        if pad > 0:\n",
    "            grad_input = [ [ row[pad:-pad] for row in layer[pad:-pad] ] for layer in [grad_input_padded] ][0]\n",
    "        else:\n",
    "            grad_input = grad_input_padded\n",
    "\n",
    "        self.grad_kernels = grad_kernels  # L∆∞u ƒë·ªÉ update sau\n",
    "        return grad_input\n",
    "\n",
    "    def step(self, learning_rate):\n",
    "        for u in range(self.kernel_size):\n",
    "            for v in range(self.kernel_size):\n",
    "                for cin_idx in range(self.in_channels):\n",
    "                    for cout_idx in range(self.out_channels):\n",
    "                        self.kernels[u][v][cin_idx][cout_idx] -= learning_rate * self.grad_kernels[u][v][cin_idx][cout_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Class ReLU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def forward(self, x):\n",
    "        self.input = x  # L∆∞u input Z ƒë·ªÉ d√πng trong backward\n",
    "        H, W, C = len(x), len(x[0]), len(x[0][0])\n",
    "        output = [[[max(0, x[i][j][c]) for c in range(C)] for j in range(W)] for i in range(H)]\n",
    "        return output\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        H, W, C = len(self.input), len(self.input[0]), len(self.input[0][0])\n",
    "        grad_input = [[[0.0 for _ in range(C)] for _ in range(W)] for _ in range(H)]\n",
    "\n",
    "        for i in range(H):\n",
    "            for j in range(W):\n",
    "                for c in range(C):\n",
    "                    grad_input[i][j][c] = grad_output[i][j][c] if self.input[i][j][c] > 0 else 0.0\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Class MaxPool2D**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool2D:\n",
    "    def __init__(self, kernel_size=2, stride=2):\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.input = x  # l∆∞u l·∫°i input cho backward\n",
    "        H_in, W_in, C = len(x), len(x[0]), len(x[0][0])\n",
    "        H_out = (H_in - self.kernel_size) // self.stride + 1\n",
    "        W_out = (W_in - self.kernel_size) // self.stride + 1\n",
    "\n",
    "        self.output = [[[0 for _ in range(C)] for _ in range(W_out)] for _ in range(H_out)]\n",
    "        self.max_indices = [[[(-1, -1) for _ in range(C)] for _ in range(W_out)] for _ in range(H_out)]\n",
    "\n",
    "        for i in range(H_out):\n",
    "            for j in range(W_out):\n",
    "                for c in range(C):\n",
    "                    max_val = float('-inf')\n",
    "                    max_idx = (0, 0)\n",
    "                    for u in range(self.kernel_size):\n",
    "                        for v in range(self.kernel_size):\n",
    "                            x_idx = i * self.stride + u\n",
    "                            y_idx = j * self.stride + v\n",
    "                            val = x[x_idx][y_idx][c]\n",
    "                            if val > max_val:\n",
    "                                max_val = val\n",
    "                                max_idx = (x_idx, y_idx)\n",
    "                    self.output[i][j][c] = max_val\n",
    "                    self.max_indices[i][j][c] = max_idx\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        H_in, W_in, C = len(self.input), len(self.input[0]), len(self.input[0][0])\n",
    "        H_out, W_out = len(d_out), len(d_out[0])\n",
    "\n",
    "        d_input = [[[0.0 for _ in range(C)] for _ in range(W_in)] for _ in range(H_in)]\n",
    "\n",
    "        for i in range(H_out):\n",
    "            for j in range(W_out):\n",
    "                for c in range(C):\n",
    "                    max_x, max_y = self.max_indices[i][j][c]\n",
    "                    d_input[max_x][max_y][c] += d_out[i][j][c]\n",
    "        return d_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Conv Block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock:\n",
    "    def __init__(self, in_channels, out_channels, num_convs=2, padding=0, stride=1):\n",
    "        self.convs = []\n",
    "        self.relus = []  # T√°ch ri√™ng ReLU cho t·ª´ng Conv\n",
    "        for _ in range(num_convs):\n",
    "            conv = Conv2D(in_channels, out_channels, kernel_size=3, padding=padding, stride=stride)\n",
    "            relu = ReLU()\n",
    "            self.convs.append(conv)\n",
    "            self.relus.append(relu)\n",
    "            in_channels = out_channels\n",
    "        self.pool = MaxPool2D(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.inputs = []  # L∆∞u t·ª´ng ƒë·∫ßu v√†o cho t·ª´ng Conv layer\n",
    "        x = input\n",
    "        for conv, relu in zip(self.convs, self.relus):\n",
    "            x = conv.forward(x)\n",
    "            self.inputs.append(x)  # L∆∞u sau m·ªói conv tr∆∞·ªõc ReLU\n",
    "            x = relu.forward(x)\n",
    "        x = self.pool.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        # Backward qua MaxPool\n",
    "        d_out = self.pool.backward(d_out)\n",
    "\n",
    "        # Backward qua c√°c Conv v√† ReLU theo th·ª© t·ª± ng∆∞·ª£c l·∫°i\n",
    "        for conv, relu in zip(reversed(self.convs), reversed(self.relus)):\n",
    "            d_out = relu.backward(d_out)\n",
    "            d_out = conv.backward(d_out)\n",
    "\n",
    "        return d_out\n",
    "    \n",
    "    def step(self, learning_rate):\n",
    "        for conv in self.convs:\n",
    "            conv.step(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Flatten**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten:\n",
    "    def forward(self, input):\n",
    "        self.input_shape = (len(input), len(input[0]), len(input[0][0]))  # (H, W, C)\n",
    "        output = []\n",
    "        for h in range(self.input_shape[0]):\n",
    "            for w in range(self.input_shape[1]):\n",
    "                for c in range(self.input_shape[2]):\n",
    "                    output.append(input[h][w][c])\n",
    "        return output\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        H, W, C = self.input_shape\n",
    "        output = [[[0.0 for _ in range(C)] for _ in range(W)] for _ in range(H)]\n",
    "        idx = 0\n",
    "        for h in range(H):\n",
    "            for w in range(W):\n",
    "                for c in range(C):\n",
    "                    output[h][w][c] = grad_output[idx]\n",
    "                    idx += 1\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Fully Connected Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnected:\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.W = [[random.uniform(-0.01, 0.01) for _ in range(out_dim)] for _ in range(in_dim)]\n",
    "        self.b = [0.0 for _ in range(out_dim)]\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        output = []\n",
    "        for k in range(self.out_dim):\n",
    "            y_k = self.b[k]\n",
    "            for d in range(self.in_dim):\n",
    "                y_k += self.W[d][k] * x[d]\n",
    "            output.append(y_k)\n",
    "        return output\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        grad_input = [0.0 for _ in range(self.in_dim)]\n",
    "        grad_W = [[0.0 for _ in range(self.out_dim)] for _ in range(self.in_dim)]\n",
    "        grad_b = [0.0 for _ in range(self.out_dim)]\n",
    "\n",
    "        for d in range(self.in_dim):\n",
    "            for k in range(self.out_dim):\n",
    "                grad_input[d] += self.W[d][k] * grad_output[k]\n",
    "\n",
    "        for d in range(self.in_dim):\n",
    "            for k in range(self.out_dim):\n",
    "                grad_W[d][k] = self.input[d] * grad_output[k]\n",
    "\n",
    "        for k in range(self.out_dim):\n",
    "            grad_b[k] = grad_output[k]\n",
    "\n",
    "        self.grad_W = grad_W\n",
    "        self.grad_b = grad_b\n",
    "\n",
    "        return grad_input\n",
    "\n",
    "    def step(self, learning_rate):\n",
    "        for d in range(self.in_dim):\n",
    "            for k in range(self.out_dim):\n",
    "                self.W[d][k] -= learning_rate * self.grad_W[d][k]\n",
    "        for k in range(self.out_dim):\n",
    "            self.b[k] -= learning_rate * self.grad_b[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **ReLU cho vector**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLUVector:\n",
    "    def forward(self, x):\n",
    "        self.input = x  # l∆∞u input ƒë·ªÉ backward\n",
    "        return [max(0, v) for v in x]\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        grad_input = []\n",
    "        for i in range(len(self.input)):\n",
    "            if self.input[i] > 0:\n",
    "                grad_input.append(grad_output[i])\n",
    "            else:\n",
    "                grad_input.append(0.0)\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Softmax + CrossEntropyLoss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def forward(self, x):\n",
    "        self.input = x  # l∆∞u input ƒë·ªÉ n·∫øu c·∫ßn debug\n",
    "        max_x = max(x)  # ch·ªëng tr√†n s·ªë\n",
    "        exps = [math.exp(v - max_x) for v in x]\n",
    "        sum_exps = sum(exps)\n",
    "        self.output = [v / sum_exps for v in exps]\n",
    "        return self.output\n",
    "\n",
    "class CrossEntropyLoss:\n",
    "    def forward(self, pred_probs, target_onehot):\n",
    "        self.pred_probs = pred_probs  # l∆∞u l·∫°i cho backward\n",
    "        self.target_onehot = target_onehot\n",
    "        loss = 0.0\n",
    "        for i in range(len(pred_probs)):\n",
    "            loss -= target_onehot[i] * math.log(pred_probs[i] + 1e-12)\n",
    "        return loss\n",
    "\n",
    "    def backward(self):\n",
    "        grad = []\n",
    "        for i in range(len(self.pred_probs)):\n",
    "            grad.append(self.pred_probs[i] - self.target_onehot[i])\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **H√†m b·ªï tr·ª£ log th√¥ng tin**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_training_details(epoch, epochs, epoch_time, n_samples, loss, accuracy, filename):\n",
    "    # T·∫°o file n·∫øu ch∆∞a c√≥\n",
    "    is_new_file = not os.path.exists(filename)\n",
    "\n",
    "    with open(filename, \"a\") as file:\n",
    "        # Ghi ti√™u ƒë·ªÅ n·∫øu file m·ªõi\n",
    "        if is_new_file:\n",
    "            file.write(\"Epoch,Total Epochs,Epoch Time (s),Time per Step (ms),Loss,Accuracy\\n\")\n",
    "\n",
    "        # T√≠nh th·ªùi gian m·ªói b∆∞·ªõc (millisecond)\n",
    "        time_per_step = (epoch_time * 1000) / n_samples\n",
    "\n",
    "        # Ghi chi ti·∫øt qu√° tr√¨nh hu·∫•n luy·ªán\n",
    "        file.write(f\"{epoch+1},{epochs},{epoch_time:.6f},{time_per_step:.6f},{loss:.6f},{accuracy:.6f}\\n\")\n",
    "\n",
    "def log_details(id, n_epochs,\n",
    "                conv2d_time, relu_time, maxpool_time,\n",
    "                flatten_time, fc_time, softmax_time,\n",
    "                filename):\n",
    "    is_new_file = not os.path.exists(filename)\n",
    "\n",
    "    with open(filename, \"a\") as file:\n",
    "        if is_new_file:\n",
    "            file.write(\"ID,Total Epochs,Conv2D Time (s),ReLU Time (s),MaxPool2D Time (s),Flatten Time (s),FullyConnected Time (s),Softmax Time (s)\\n\")\n",
    "\n",
    "        file.write(f\"{id},{n_epochs},\"\n",
    "                   f\"{conv2d_time:.6f},{relu_time:.6f},{maxpool_time:.6f},\"\n",
    "                   f\"{flatten_time:.6f},{fc_time:.6f},{softmax_time:.6f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Class C·∫•u tr√∫c**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN:\n",
    "    def __init__(self):\n",
    "        self.block1 = ConvBlock(3, 64, num_convs=2, padding=1)\n",
    "        self.block2 = ConvBlock(64, 128, num_convs=2, padding=1)\n",
    "        self.block3 = ConvBlock(128, 256, num_convs=2, padding=1)\n",
    "        self.flatten = Flatten()\n",
    "        self.fc1 = FullyConnected(256 * 4 * 4, 512)  # Sau 3 l·∫ßn pooling 2x2, ·∫£nh c√≤n 4x4\n",
    "        self.relu_fc1 = ReLUVector()\n",
    "        self.fc2 = FullyConnected(512, 10)\n",
    "        self.softmax = Softmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, train_labels, epochs=10, learning_rate=0.01, mb_size=32, \n",
    "          log_filename=\"training_log_python.csv\", detail_log_filename=\"details_log_python.csv\"):\n",
    "    random.seed(0)\n",
    "    cross_entropy = CrossEntropyLoss()\n",
    "\n",
    "    n_samples = len(train_data)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start_epoch = time.time()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "\n",
    "        conv2d_time = relu_time = maxpool_time = flatten_time = fc_time = softmax_time = 0.0\n",
    "\n",
    "        # Shuffle data\n",
    "        indices = list(range(n_samples))\n",
    "        random.shuffle(indices)\n",
    "        train_data_shuffled = [train_data[i] for i in indices]\n",
    "        train_labels_shuffled = [train_labels[i] for i in indices]\n",
    "\n",
    "        # Mini-batch training\n",
    "        full_batch_count = n_samples // mb_size\n",
    "        for i in range(full_batch_count):\n",
    "            start = i * mb_size\n",
    "            end = start + mb_size\n",
    "            X_batch = train_data_shuffled[start:end]\n",
    "            y_batch = train_labels_shuffled[start:end]\n",
    "\n",
    "            # Train each sample in batch\n",
    "            for x, target in zip(X_batch, y_batch):\n",
    "                ## Forward pass\n",
    "                t0 = time.time()\n",
    "                out = model.block1.forward(x)\n",
    "                t1 = time.time()\n",
    "                conv2d_time += t1 - t0\n",
    "\n",
    "                t0 = time.time()\n",
    "                out = model.block2.forward(out)\n",
    "                t1 = time.time()\n",
    "                conv2d_time += t1 - t0\n",
    "\n",
    "                t0 = time.time()\n",
    "                out = model.block3.forward(out)\n",
    "                t1 = time.time()\n",
    "                conv2d_time += t1 - t0\n",
    "\n",
    "                t0 = time.time()\n",
    "                out = model.flatten.forward(out)\n",
    "                t1 = time.time()\n",
    "                flatten_time += t1 - t0\n",
    "\n",
    "                t0 = time.time()\n",
    "                out = model.fc1.forward(out)\n",
    "                t1 = time.time()\n",
    "                fc_time += t1 - t0\n",
    "\n",
    "                t0 = time.time()\n",
    "                out = model.relu_fc1.forward(out)\n",
    "                t1 = time.time()\n",
    "                relu_time += t1 - t0\n",
    "\n",
    "                t0 = time.time()\n",
    "                out = model.fc2.forward(out)\n",
    "                t1 = time.time()\n",
    "                fc_time += t1 - t0\n",
    "\n",
    "                t0 = time.time()\n",
    "                pred_probs = model.softmax.forward(out)\n",
    "                t1 = time.time()\n",
    "                softmax_time += t1 - t0\n",
    "\n",
    "                loss = cross_entropy.forward(pred_probs, target)\n",
    "                total_loss += loss\n",
    "\n",
    "                pred_label = pred_probs.index(max(pred_probs))\n",
    "                true_label = target.index(max(target))\n",
    "                if pred_label == true_label:\n",
    "                    correct += 1\n",
    "\n",
    "                ## Backward pass\n",
    "                grad = [pred_probs[i] - target[i] for i in range(len(target))]\n",
    "\n",
    "                grad = model.fc2.backward(grad)\n",
    "                grad = model.relu_fc1.backward(grad)\n",
    "                grad = model.fc1.backward(grad)\n",
    "                grad = model.flatten.backward(grad)\n",
    "                grad = model.block3.backward(grad)\n",
    "                grad = model.block2.backward(grad)\n",
    "                grad = model.block1.backward(grad)\n",
    "\n",
    "                ## Update weights\n",
    "                model.fc2.step(learning_rate)\n",
    "                model.fc1.step(learning_rate)\n",
    "                model.block3.step(learning_rate)\n",
    "                model.block2.step(learning_rate)\n",
    "                model.block1.step(learning_rate)\n",
    "\n",
    "        # End of epoch\n",
    "        end_epoch = time.time()\n",
    "        epoch_time = end_epoch - start_epoch\n",
    "        accuracy = correct / (full_batch_count * mb_size)\n",
    "\n",
    "        # Logging\n",
    "        log_training_details(epoch, epochs, epoch_time, n_samples, total_loss / (full_batch_count * mb_size), accuracy, log_filename)\n",
    "        log_details(f\"Epoch_{epoch+1}\", mb_size, conv2d_time, relu_time, maxpool_time, flatten_time, fc_time, softmax_time, detail_log_filename)\n",
    "\n",
    "        # Print\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss/(full_batch_count * mb_size):.4f} - Acc: {accuracy:.4f} - Time: {epoch_time:.2f}s\")\n",
    "        print(f\"    Conv2D: {conv2d_time:.2f}s | ReLU: {relu_time:.2f}s | MaxPool: {maxpool_time:.2f}s | Flatten: {flatten_time:.2f}s | FC: {fc_time:.2f}s | Softmax: {softmax_time:.2f}s\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = load_cifar10_data()\n",
    "\n",
    "# Convert numpy arrays to pure Python lists\n",
    "X_train = X_train.tolist()\n",
    "y_train = y_train.tolist()\n",
    "X_test = X_test.tolist()\n",
    "y_test = y_test.tolist()\n",
    "\n",
    "# Convert from (3, 32, 32) -> (32, 32, 3)\n",
    "def transpose_channels(img):\n",
    "    C, H, W = len(img), len(img[0]), len(img[0][0])\n",
    "    transposed = [[[0.0 for _ in range(C)] for _ in range(W)] for _ in range(H)]\n",
    "    for c in range(C):\n",
    "        for h in range(H):\n",
    "            for w in range(W):\n",
    "                transposed[h][w][c] = img[c][h][w]\n",
    "    return transposed\n",
    "\n",
    "X_train = [transpose_channels(img) for img in X_train]\n",
    "X_test = [transpose_channels(img) for img in X_test]\n",
    "\n",
    "# Normalize pixel values to [0, 1]\n",
    "def normalize_image(img):\n",
    "    H, W, C = len(img), len(img[0]), len(img[0][0])\n",
    "    for h in range(H):\n",
    "        for w in range(W):\n",
    "            for c in range(C):\n",
    "                img[h][w][c] /= 255.0\n",
    "    return img\n",
    "\n",
    "X_train = [normalize_image(img) for img in X_train]\n",
    "X_test = [normalize_image(img) for img in X_test]\n",
    "\n",
    "# One-hot encoding labels\n",
    "def one_hot_encode(labels, num_classes=10):\n",
    "    one_hot = []\n",
    "    for label in labels:\n",
    "        vec = [0.0] * num_classes\n",
    "        vec[label] = 1.0\n",
    "        one_hot.append(vec)\n",
    "    return one_hot\n",
    "\n",
    "y_train = one_hot_encode(y_train)\n",
    "y_test = one_hot_encode(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **1 epoch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 - Loss: 2.3312 - Acc: 0.0000 - Time: 974.11s\n",
      "    Conv2D: 260.58s | ReLU: 0.00s | MaxPool: 0.00s | Flatten: 0.00s | FC: 3.05s | Softmax: 0.00s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = SimpleCNN()\n",
    "\n",
    "train(model, X_train[:10], y_train[:10], epochs=1, learning_rate=0.01, mb_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Nh·∫≠n x√©t**\n",
    "- ``Python thu·∫ßn`` x·ª≠ l√Ω t·ª´ng pixel, t·ª´ng ph√©p nh√¢n c·ªông ‚ûî t·ªëc ƒë·ªô r·∫•t ch·∫≠m (kh√¥ng t·ªëi ∆∞u h√≥a v√≤ng l·∫∑p nh∆∞ NumPy/C).\n",
    "- D·ªØ li·ªáu ·∫£nh 32x32x3, kh√° nhi·ªÅu ph√©p to√°n cho m·ªói ·∫£nh.\n",
    "- V·ªõi Python thu·∫ßn, hu·∫•n luy·ªán CNN ngay c·∫£ v·ªõi 10 ·∫£nh c≈©ng qu√° ch·∫≠m ƒë·ªÉ √°p d·ª•ng th·ª±c t·∫ø.\n",
    "- Ch·∫°y ƒë·∫ßy ƒë·ªß 50k ·∫£nh CIFAR-10 theo c√°ch n√†y s·∫Ω **m·∫•t h√†ng th√°ng**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **ƒê·ªãnh h∆∞·ªõng ti·∫øp theo**\n",
    "- 1 verson code s·ª≠ d·ª•ng Numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Phi√™n b·∫£n ƒë∆∞·ª£c vi·∫øt b·∫±ng Numpy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Class Conv2D**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D_Numpy:\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=0, stride=1):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "\n",
    "        # Kh·ªüi t·∫°o kernels d·∫°ng NumPy array\n",
    "        self.kernels = np.random.uniform(-0.1, 0.1, \n",
    "            (kernel_size, kernel_size, in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        self.input_data = input_data\n",
    "        pad = self.padding\n",
    "        stride = self.stride\n",
    "        k = self.kernel_size\n",
    "\n",
    "        # Zero-padding input\n",
    "        input_padded = np.pad(input_data, \n",
    "            ((pad, pad), (pad, pad), (0, 0)), \n",
    "            mode='constant', constant_values=0\n",
    "        )\n",
    "        self.input_padded = input_padded\n",
    "\n",
    "        H, W, _ = input_padded.shape\n",
    "        out_h = (H - k) // stride + 1\n",
    "        out_w = (W - k) // stride + 1\n",
    "\n",
    "        output = np.zeros((out_h, out_w, self.out_channels))\n",
    "\n",
    "        for i in range(out_h):\n",
    "            for j in range(out_w):\n",
    "                region = input_padded[i*stride:i*stride+k, j*stride:j*stride+k, :]  # (k, k, cin)\n",
    "                for cout_idx in range(self.out_channels):\n",
    "                    output[i, j, cout_idx] = np.sum(region * self.kernels[:, :, :, cout_idx])\n",
    "        return output\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        pad = self.padding\n",
    "        stride = self.stride\n",
    "        k = self.kernel_size\n",
    "        cin = self.in_channels\n",
    "        cout = self.out_channels\n",
    "        input_padded = self.input_padded\n",
    "\n",
    "        H, W, _ = input_padded.shape\n",
    "        out_h, out_w, _ = grad_output.shape\n",
    "\n",
    "        grad_input_padded = np.zeros_like(input_padded)\n",
    "        grad_kernels = np.zeros_like(self.kernels)\n",
    "\n",
    "        for i in range(out_h):\n",
    "            for j in range(out_w):\n",
    "                region = input_padded[i*stride:i*stride+k, j*stride:j*stride+k, :]\n",
    "                for cout_idx in range(cout):\n",
    "                    grad_kernels[:, :, :, cout_idx] += region * grad_output[i, j, cout_idx]\n",
    "                    grad_input_padded[i*stride:i*stride+k, j*stride:j*stride+k, :] += self.kernels[:, :, :, cout_idx] * grad_output[i, j, cout_idx]\n",
    "\n",
    "        # B·ªè padding\n",
    "        if pad > 0:\n",
    "            grad_input = grad_input_padded[pad:-pad, pad:-pad, :]\n",
    "        else:\n",
    "            grad_input = grad_input_padded\n",
    "\n",
    "        self.grad_kernels = grad_kernels\n",
    "        return grad_input\n",
    "\n",
    "    def step(self, learning_rate):\n",
    "        self.kernels -= learning_rate * self.grad_kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Class Relu**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU_Numpy:\n",
    "    def forward(self, x):\n",
    "        self.input = x  # L∆∞u l·∫°i ƒë·ªÉ backward\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        grad_input = grad_output.copy()\n",
    "        grad_input[self.input <= 0] = 0\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Class MaxPool2D**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool2D_Numpy:\n",
    "    def __init__(self, kernel_size=2, stride=2):\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        H_in, W_in, C = x.shape\n",
    "        H_out = (H_in - self.kernel_size) // self.stride + 1\n",
    "        W_out = (W_in - self.kernel_size) // self.stride + 1\n",
    "        \n",
    "        # Reshape\n",
    "        x_reshaped = x.reshape(\n",
    "            H_out, self.stride, \n",
    "            W_out, self.stride, \n",
    "            C\n",
    "        )\n",
    "        # Swap axes ƒë·ªÉ gom l·∫°i th√†nh √¥ pooling\n",
    "        x_reshaped = x_reshaped.transpose(0, 2, 1, 3, 4)  # (H_out, W_out, stride, stride, C)\n",
    "\n",
    "        # Max pooling\n",
    "        output = np.max(x_reshaped, axis=(2,3))\n",
    "\n",
    "        # L∆∞u l·∫°i ƒë·ªÉ backward\n",
    "        self.x_reshaped = x_reshaped\n",
    "        self.output = output\n",
    "        return output\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        H_out, W_out, C = d_out.shape\n",
    "        d_x_reshaped = np.zeros_like(self.x_reshaped)\n",
    "\n",
    "        # T·∫°o mask v·ªã tr√≠ max\n",
    "        max_mask = (self.x_reshaped == self.output[:, :, None, None, :])\n",
    "\n",
    "        # Ph√¢n ph·ªëi gradient ƒë√∫ng v√†o ch·ªó max\n",
    "        d_x_reshaped[max_mask] = d_out.repeat(self.stride, axis=0).repeat(self.stride, axis=1).flatten()\n",
    "\n",
    "        # Chuy·ªÉn l·∫°i th√†nh (H_in, W_in, C)\n",
    "        H_in = H_out * self.stride\n",
    "        W_in = W_out * self.stride\n",
    "        d_input = d_x_reshaped.reshape(H_in, W_in, C)\n",
    "        return d_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Conv Block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock_Numpy:\n",
    "    def __init__(self, in_channels, out_channels, num_convs=2, padding=0, stride=1):\n",
    "        self.convs = []\n",
    "        self.relus = []\n",
    "        for _ in range(num_convs):\n",
    "            conv = Conv2D(in_channels, out_channels, kernel_size=3, padding=padding, stride=stride)\n",
    "            relu = ReLU()\n",
    "            self.convs.append(conv)\n",
    "            self.relus.append(relu)\n",
    "            in_channels = out_channels\n",
    "        self.pool = MaxPool2D_Numpy(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.inputs = []  # L∆∞u input tr∆∞·ªõc m·ªói Conv\n",
    "        for conv, relu in zip(self.convs, self.relus):\n",
    "            self.inputs.append(x)  # L∆∞u input tr∆∞·ªõc khi v√†o Conv\n",
    "            x = conv.forward(x)\n",
    "            x = relu.forward(x)\n",
    "        x = self.pool.forward(np.array(x))  # Convert list -> np.array cho MaxPool2D_Numpy\n",
    "        self.output = x\n",
    "        return x\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        d_out = self.pool.backward(d_out)\n",
    "\n",
    "        for conv, relu in zip(reversed(self.convs), reversed(self.relus)):\n",
    "            d_out = relu.backward(d_out)\n",
    "            d_out = conv.backward(d_out)\n",
    "\n",
    "        return d_out\n",
    "\n",
    "    def step(self, learning_rate):\n",
    "        for conv in self.convs:\n",
    "            conv.step(learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Flatten**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten_Numpy:\n",
    "    def forward(self, x):\n",
    "        self.input_shape = x.shape  # (H, W, C)\n",
    "        return x.flatten()\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        return grad_output.reshape(self.input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Fully Connected Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnected_Numpy:\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.W = np.random.uniform(-0.01, 0.01, (in_dim, out_dim))\n",
    "        self.b = np.zeros(out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.input = x  # x shape: (in_dim,)\n",
    "        return np.dot(x, self.W) + self.b  # output shape: (out_dim,)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        # grad_output shape: (out_dim,)\n",
    "        grad_input = np.dot(self.W, grad_output)  # (in_dim,)\n",
    "        self.grad_W = np.outer(self.input, grad_output)  # (in_dim, out_dim)\n",
    "        self.grad_b = grad_output  # (out_dim,)\n",
    "        return grad_input\n",
    "\n",
    "    def step(self, learning_rate):\n",
    "        self.W -= learning_rate * self.grad_W\n",
    "        self.b -= learning_rate * self.grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Relu cho Vector**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLUVector_Numpy:\n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        grad_input = grad_output * (self.input > 0)\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Softmax + CrossEntropyLoss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax_Numpy:\n",
    "    def forward(self, x):\n",
    "        self.input = x  \n",
    "        max_x = np.max(x) \n",
    "        exps = np.exp(x - max_x)\n",
    "        sum_exps = np.sum(exps)\n",
    "        self.output = exps / sum_exps\n",
    "        return self.output\n",
    "\n",
    "class CrossEntropyLoss_Numpy:\n",
    "    def forward(self, pred_probs, target_onehot):\n",
    "        self.pred_probs = pred_probs\n",
    "        self.target_onehot = target_onehot\n",
    "        loss = -np.sum(target_onehot * np.log(pred_probs + 1e-12))\n",
    "        return loss\n",
    "\n",
    "    def backward(self):\n",
    "        return self.pred_probs - self.target_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Class C·∫•u Tr√∫c**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN_Numpy:\n",
    "    def __init__(self):\n",
    "        self.block1 = ConvBlock_Numpy(3, 64, num_convs=2, padding=1)\n",
    "        self.block2 = ConvBlock_Numpy(64, 128, num_convs=2, padding=1)\n",
    "        self.block3 = ConvBlock_Numpy(128, 256, num_convs=2, padding=1)\n",
    "        self.flatten = Flatten_Numpy()\n",
    "        self.fc1 = FullyConnected_Numpy(256 * 4 * 4, 512)  # Sau 3 l·∫ßn pooling 2x2 -> c√≤n 4x4\n",
    "        self.relu_fc1 = ReLUVector_Numpy()\n",
    "        self.fc2 = FullyConnected_Numpy(512, 10)\n",
    "        self.softmax = Softmax_Numpy()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1.forward(x)\n",
    "        x = self.block2.forward(x)\n",
    "        x = self.block3.forward(x)\n",
    "        x = self.flatten.forward(x)\n",
    "        x = self.fc1.forward(x)\n",
    "        x = self.relu_fc1.forward(x)\n",
    "        x = self.fc2.forward(x)\n",
    "        x = self.softmax.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        grad = self.softmax.backward(grad_output)\n",
    "        grad = self.fc2.backward(grad)\n",
    "        grad = self.relu_fc1.backward(grad)\n",
    "        grad = self.fc1.backward(grad)\n",
    "        grad = self.flatten.backward(grad)\n",
    "        grad = self.block3.backward(grad)\n",
    "        grad = self.block2.backward(grad)\n",
    "        grad = self.block1.backward(grad)\n",
    "        return grad\n",
    "\n",
    "    def step(self, learning_rate):\n",
    "        self.block1.step(learning_rate)\n",
    "        self.block2.step(learning_rate)\n",
    "        self.block3.step(learning_rate)\n",
    "        self.fc1.step(learning_rate)\n",
    "        self.fc2.step(learning_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
